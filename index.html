
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/">
        <link rel="stylesheet" href="./jemdoc.css" type="text/css">
        <link rel="shortcut icon" href="./fig/favicon.ico" type="image/x-icon" />.
        <title>Jiamin LI@CityU HK - CS</title>
</head>

<body data-gr-c-s-loaded="true">
        <script>
                function myFunction1() {
                        var dots = document.getElementById("dots1");
                        var moreText = document.getElementById("more1");
                        var btnText = document.getElementById("myBtn1");

                        if (dots.style.display === "none") {
                        dots.style.display = "inline";
                        btnText.innerHTML = "more"; 
                        moreText.style.display = "none";
                        } else {
                        dots.style.display = "none";
                        btnText.innerHTML = "less"; 
                        moreText.style.display = "inline";
                        }
                }
                function myFunction2() {
                        var dots = document.getElementById("dots2");
                        var moreText = document.getElementById("more2");
                        var btnText = document.getElementById("myBtn2");

                        if (dots.style.display === "none") {
                        dots.style.display = "inline";
                        btnText.innerHTML = "more"; 
                        moreText.style.display = "none";
                        } else {
                        dots.style.display = "none";
                        btnText.innerHTML = "less"; 
                        moreText.style.display = "inline";
                        }
                }
        </script>
        <div id="layout-content">
                <!-- <p><img src="./res/portait_clearwater_bay.jpg" width="380px" align="right" style="margin:8px"></p> -->
                <h1 style="margin-bottom:30px">Jiamin LI (李嘉敏)</h1>
                <p>she/her/hers</p>
                <p><b>Ph.D. Candidate</b></p>
                <p><a href="https://www.cs.cityu.edu.hk/" target="_blank">Department of Computer Science</a><br>
                        <a href="https://www.cityu.edu.hk/" target="_blank">City University of Hong Kong</a>
                </p>
                <p><b>Email</b>: jiaminli.icy [at] gmail.com<br></p>
                <p><b><a href="./cv_JiaminLI.pdf" target="_blank">CV</a></b> (I usually keep my CV up-to-date.)</p>
                <a href="https://www.linkedin.com/in/jiamin-li-9998ab129/" target="_blank"><img src="./fig/LinkedIn_s.png"
                                height="30px"></a>
                <a href="https://github.com/serendipityCoding" target="_blank"><img src="./fig/github_s.jpg" height="30px"></a>

                <h2>Biography <font face="georgia, serif" size="3"></font>
                </h2>
                <p>I am a fourth-year Ph.D. candidate at <a href="https://www.cityu.edu.hk/" target="_blank">City University of Hong Kong</a>, advised by <a
                                href="https://henryhxu.github.io/" target="_blank">Prof. Hong Xu, Henry</a> and 
                                <a
                                href="https://www.cs.cityu.edu.hk/~congwang/" target="_blank">Prof. Cong Wang</a>.
                        Previously, I obtained my bachelor's degree in Computer Science from <a
                                href="https://www.cityu.edu.hk/" target="_blank">City University of Hong Kong</a> in 2019. My research
                        interests lie primarily in the area of distributed machine learning system (MLSys).
                </p>
                <p>
                        <b>I am on the job market with expect graduation in Spring 2024, hopefully :)</b>
                </p> 
                <h2>Research Interests <font face="georgia, serif" size="3">
                    
                </font>
                </h2>
                <li>
                    Distributed machine learning system: accelerating large-scale distributed DNN tasks<p></p>
                </li>
                <li>
                    Adaptive and sparse computation: exploring new computing paradigms to keep scaling DNN models<p></p>
                </li>
                <li>
                    Simulation: building accurate performance simulator of DNN training workloads<p></p>
                </li>
                <li>
                    Resource scheduling in GPU clusters: designing efficient scheduling algorithms for DNN tasks <p></p>
                </li>
                <h2>Publications <font face="georgia, serif" size="3">
                                <!-- (<a
                                        href="http://dblp.uni-trier.de/pers/hd/p/Pui:Chak=Wa"><img
                                                src="./res/dblp-5.png" height="30px" style="margin-bottom:-7px"></a>, <a
                                        href="https://scholar.google.com.hk/citations?user=5pqag5YAAAAJ&amp;hl=en"><img
                                                src="./Chak-Wa Pui@CUHK-CSE_files/google-scholar.png" height="20px"
                                                style="margin-bottom:-4px"></a>) -->
                        </font>
                </h2>

                <h3>Journal Paper</h3>
                <ul>
                        <li>
                                <p>[J2] Libin Liu, Hong Xu, Zhixiong Niu, Jingzong Li, Wei Zhang, Peng Wang, <b>Jiamin Li</b>, Jason Xue Chun, Cong Wang,
                                        <a href="" target="_blank">"ScaleFlux: Efficient Stateful Scaling in NFV"</a>,
                                        IEEE Transactions on Parallel and Distributed Systems, 2022. </p>
                        </li>
                        <li>
                                <p>[J1] Libin Liu, Chengxi Gao, Peng Wang, Hongming Huang, <b>Jiamin Li</b>, Hong Xu, Wei Zhang,
                                        <a href="https://ieeexplore.ieee.org/document/9616443" target="_blank">"Bottleneck-Aware Non-Clairvoyant Coflow Scheduling with Fai"</a>,
                                        IEEE Transactions on Cloud Computing, 2021. </p>
                        </li>
                </ul>
                <h3>Conference Paper</h3>
                <ul>
                        <li>                                
                                <p>[C3] <b>Jiamin Li</b>, Yimin Jiang, Yibo Zhu, Cong Wang, Hong Xu,
                                        <a href="https://serendipitycoding.github.io/" target="_blank">"Accelerating Distributed MoE Training and Inference with Lina"</a>,
	                                        USENIX Annual Technical Conference&nbsp; (ATC), 2023.</p>
                        </li>
                        <li>                                
                                <p>[C2] <b>Jiamin Li</b>, Hong Xu, Yibo Zhu, Zherui Liu, Chuanxiong Guo, Cong Wang,
                                        <a href="https://serendipitycoding.github.io/" target="_blank">"Lyra: Elastic Scheduling for Deep Learning Clusters"</a>, ACM European Conference on Computer Systems&nbsp;(EuroSys), 2023.</p>
                        </li>
                        <li>                                
                                <p>[C1] Kaiwei Mo, Chen Chen, <b>Jiamin Li</b>, Hong Xu, Chun Jason Xue,
                                        <a href="https://ieeexplore.ieee.org/document/9533708" target="_blank">"Two-Dimensional Learning Rate Decay: Towards Accurate Federated Learning with Non-IID Data"</a>, IEEE International Joint Conference on Neural Networks&nbsp;(IJCNN), 2021.</p>
                        </li>

                </ul>
<!--                 <h3>Arxiv</h3>
                <ul>
                        <li>
                                                        </li>

                </ul> -->
                <h2>Experiences</h2>
                <p>Visiting Student Reseacher @<a href="https://utns.cs.utexas.edu/index.html" target="_blank">The University of Texas at Austin</a></p>
                <ul>
                        <li>
                        <p>Supervised by <a href="https://www.cs.utexas.edu/~akella/" target="_blank">Prof. Aditya Akella</a>.</p>
                        </li>
                        <li>
                                <p>WIP</p>
                        </li>
                        <li>
                                <p>Apr 2023 - Dec 2023, Austin, TX, United States </p>
                        </li>
                </ul>
                <p>Research Intern @<a href="https://www.bytedance.com/en/" target="_blank">ByteDance</a></p>
                <ul>
                        <li>
                        <p>Supervised by <a href="http://yibozhu.com/" target="_blank">Dr. Yibo Zhu</a>.</p>
                        </li>
                        <li>
                                <p>We design and implement Lyra, an elastic GPU cluster scheduler for deep learning to address these problems. The key idea is to exploit cluster-level elasticity by loaning idle inferences servers for training, and job-level elasticity by scaling jobs to better utilize the dynamic resource pool.</p>
                        </li>
                        <li>
                                <p>May 2019 - May 2021, Beijing, Shenzhen </p>
                        </li>
                </ul>
                <p>Part-time Research Assistant @<a href="https://www.cityu.edu.hk" target="_blank">CityU</a></p>
                <ul>
                        <li>
                                <p>Supervised by <a href="https://henryhxu.github.io" target="_blank">Prof. Henry Xu</a>.</p>
                        </li>
                        <li>
                                <p>We aim to accelerate the PS distributed training system by reducing the transfer size of each communication operation, develop a control knob that schedules send and receive operations in PS distributed training system.</p>
                        </li>
                        <li>
                                <p>June 2018 - May 2019, HKSAR </p>
                        </li>
                </ul>
                <p>Software Developer @<a href="https://www.jardines.com/en" target="_blank">Jardine Matheson</a></p>
                <ul>
                        <li>
                                <p>Design and develop web services for employee recruitment in Group Human Resources department.</p>
                        </li>
                        <li>
                                <p>May 2017 - May 2018, HKSAR </p>
                        </li>
                </ul>
                <h2>Projects</h2>
                <h4>Accelerating Mixture-of-Experts models (ongoing)</h4>
                <ul>
                        <li> <p> Collaborated with ByteDance.</p> </li>
                        <li> <p> Lina, a new system that accelerates all-to-all in distributed training of large MoE models. Our idea is to combine priority-based micro-op communication scheduling with pipeline-driven expert packing.</p></li>
                        <li><p> Abstract: Scaling model parameters usually improves model quality, but at the price of high computation overhead. Sparsely activated models, usually in the form of Mixture of Experts (MoE) architecture, have constant computation cost over their dense counterparts, thus providing opportunities to train and serve a large model at a reasonable cost.<span id="dots2">..</span><span id="more2"> However, the distributed training of an MoE model is prone to low efficiency, mainly due to the interleaved all-to-all communication during model computation. 
                        This project makes three main contributions. First, we systematically analyze the all-to-all overhead in distributed training of MoE. Second, we propose a new communication scheduling scheme based on tensor partitioning that prioritizes the all-to-all operations over other communication, due to its blocking nature. Third, we introduce expert packing that reduces the all-to-all transfer size and incorporates optimizations to mitigate its overheads. Both techniques effectively tackle the all-to-all bottleneck, and we integrate them into a new system called Lina. Experiments on an A100 GPU testbed show that Lina improves the training step time of popular NLP models by up to 1.73x over the state-of-the-art.</span> <button onclick="myFunction2()" id="myBtn2">more</button></p>
                        </li>
                </ul>
                <h4>Simulation of large-scale distributed DNN training tasks (ongoing)</h4>
                <ul>
                        <li> <p>Collaborated with Microsoft Research Asia</p></li>
                        <li> Merak, a simulation toolkit for estimating the step time of large-scale distributed DNN model. It can accurately simulate the communication operations in distributed training and the interference between communication operations and computation operations. </li>
                </ul>
                <h4>DNN Training Job Scheduling</h4>
                <ul>
                        <li> <p>Work completed at ByteDance. </p></li>
                        <li><p>Lyra, an elastic GPU cluster scheduler for deep learning. The key idea is to exploit cluster-level elasticity by loaning idle inferences servers for training, and job-level elasticity by scaling jobs to better utilize the dynamic resource pool.</p></li>
                        <li><p>Abstract: Organizations build separate training and inference GPU clusters for deep learning, and use separate schedulers to manage them. This leads to problems for both: inference clusters have low GPU utilization when the traffic load is low; training jobs often experience long queuing due to lack of resources.<span id="dots1">..</span><span id="more1"> We introduce Lyra, a new cluster scheduler to address these problems. Lyra introduces capacity loaning to loan idle inference GPU servers for training jobs. It further exploits elastic scaling that scales a training job’s GPU allocation to better utilize loaned resources. Capacity loaning and elastic scaling create new challenges to cluster management. When the loaned servers need to be returned, we need to minimize job preemptions; when more GPUs become available, we need to allocate them to elastic jobs and minimize the job completion time (JCT). Lyra addresses these combinatorial problems with principled heuristics. It introduces the notion of server preemption cost which it greedily reduces during server reclaiming. It further relies on the JCT reduction value defined for each additional worker for an elastic job to solve the scheduling problem as a multiple-choice knapsack problem. Prototype implementation on a 64-GPU testbed and large-scale simulation with 15-day traces of over 50,000 production jobs show that Lyra brings 1.53x and 1.50x reductions in average queuing time and JCT, and improves cluster usage by up to 26.9%. </span><button onclick="myFunction1()" id="myBtn1">more</button></p>

                </ul>
                

                <h2>Teaching <font face="georgia, serif" size="3">
                    
                </font>
                </h2>
                <li>
                        CS2311, Computer Programming, Sem A, 2022/2023<p></p>
                </li>
                <li>
                        CS4296 & CS5296, Cloud Computing, Sem B, 2021/2022<p></p>
                </li>
                <li>
                        CS4394 & CS5294, Information Security and Management, Sem A, 2021/2022<p></p>
                </li>
                <li>
                        CS4293 & CS6290, Topics on Computer Security, Sem B, 2020/2021<p></p>
                </li>
                <li>
                        CS5222, Computer Networks and Internets, Sem A, 2020/2021<p></p>
                </li>
    
                <li>
                    CS4296 & CS5296, Cloud Computing, Sem B, 2019/2020<p></p>
                </li>
                <li>
                CS2311, Computer Programming, Sem A, 2019/2020<p></p>
                </li>
                <h2></h2>
                
        </div>
        <div hidden>
                <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=OTh-QdjbO0HYyOxhZs0T4JprmTO08X7UFB_eit1qyZ4&cl=ffffff&w=a"></script>
        </div>
        </body>
</html>
